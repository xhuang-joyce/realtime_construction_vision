import asyncio
import time
import threading
import logging
import json
from typing import Optional, Callable, Dict, Any
from datetime import datetime
import queue
import cv2
import base64

from .camera_capture import CameraCapture
from .screen_capture import ScreenCapture
from .microphone_capture import MicrophoneCapture
from .openai_realtime import OpenAIRealtimeClient
from .audio_handler import AudioResponseHandler

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealtimeConversationAgent:
    """
    çœŸæ­£çš„å®æ—¶å¯¹è¯ä»£ç† - æ”¯æŒæŒç»­è¯­éŸ³å¯¹è¯å’Œè§†è§‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥
    True Realtime Conversation Agent with continuous voice chat and visual context awareness
    """
    
    def __init__(self, 
                 api_key: str,
                 camera_index: int = 0,
                 microphone_index: Optional[int] = None,
                 visual_context_interval: int = 10,
                 input_source: str = "camera"):  # "camera" or "screen"
        """
        Initialize realtime conversation agent
        
        Args:
            api_key: OpenAI API key
            camera_index: Camera device index
            microphone_index: Microphone device index (None for default)
            visual_context_interval: Update visual context every N seconds
            input_source: "camera" for camera input, "screen" for screen capture
        """
        self.api_key = api_key
        self.visual_context_interval = visual_context_interval
        self.input_source = input_source
        
        # Initialize visual input component based on source
        if input_source == "screen":
            self.visual_input = ScreenCapture(fps=15)
            logger.info("Using screen capture as visual input")
        else:
            self.visual_input = CameraCapture(camera_index)
            logger.info(f"Using camera {camera_index} as visual input")
        
        # Initialize other components
        self.microphone = MicrophoneCapture()  # device_index passed in start_capture()
        self.microphone_index = microphone_index  # Save device index
        self.ai_client = OpenAIRealtimeClient(api_key)
        self.audio_handler = AudioResponseHandler(api_key)
        
        # çŠ¶æ€ç®¡ç†
        self.is_running = False
        self.conversation_active = False
        self.last_visual_update = 0
        self.current_visual_context = None
        
        # å­˜å‚¨ä¸»äº‹ä»¶å¾ªç¯å¼•ç”¨
        self.main_loop = None
        
        # éŸ³é¢‘ç¼“å†²ç®¡ç†
        self.audio_buffer = []
        self.min_audio_duration_ms = 100  # æœ€å°‘100mséŸ³é¢‘
        self.audio_sample_rate = 24000  # 24kHz
        self.min_audio_samples = int(self.min_audio_duration_ms * self.audio_sample_rate / 1000)
        self.current_audio_samples = 0
        
        # é˜²æ­¢é‡å¤è¯·æ±‚
        self.is_processing_audio = False
        
        # çº¿ç¨‹å’Œä»»åŠ¡
        self.visual_update_task = None
        self.conversation_task = None
        
        # å›è°ƒè®¾ç½®
        self._setup_callbacks()
        
        logger.info("Realtime Conversation Agent initialized")
    
    def _setup_callbacks(self):
        """è®¾ç½®å„ç§å›è°ƒå‡½æ•°"""
        
        # AI å®¢æˆ·ç«¯å›è°ƒ
        self.ai_client.set_text_callback(self._handle_ai_text_response)
        self.ai_client.set_audio_callback(self._handle_ai_audio_response)
        self.ai_client.set_error_callback(self._handle_ai_error)
        
        # éº¦å…‹é£å›è°ƒ
        self.microphone.set_speech_callbacks(
            on_start=self._handle_speech_start,
            on_end=self._handle_speech_end
        )
        self.microphone.set_audio_callback(self._handle_audio_data)
        
        logger.info("Callbacks configured")
    
    async def start_conversation(self):
        """å¼€å§‹å®æ—¶å¯¹è¯"""
        try:
            logger.info("Starting realtime conversation agent...")
            
            # ä¿å­˜å½“å‰äº‹ä»¶å¾ªç¯å¼•ç”¨
            self.main_loop = asyncio.get_running_loop()
            
            # Start visual input (camera or screen)
            if self.input_source == "screen":
                self.visual_input.start_capture()
                logger.info("Screen capture started")
            else:
                if not self.visual_input.start_capture():
                    raise Exception("Failed to start camera")
            
            # å¯åŠ¨éº¦å…‹é£
            if not self.microphone.start_capture(device_index=self.microphone_index):
                raise Exception("Failed to start microphone")
            
            # ä¼˜åŒ–VADè®¾ç½®ä»¥å‡å°‘å»¶è¿Ÿ
            self.microphone.set_vad_threshold(300)  # é™ä½é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿ
            logger.info("VAD optimized for lower latency")
            
            # å¯åŠ¨éŸ³é¢‘æ’­æ”¾
            self.audio_handler.start()
            
            # è¿æ¥åˆ° OpenAI Realtime API
            if not await self.ai_client.connect():
                raise Exception("Failed to connect to OpenAI Realtime API")
            
            self.is_running = True
            
            # å¯åŠ¨åå°ä»»åŠ¡
            self.visual_update_task = asyncio.create_task(self._visual_context_loop())
            self.conversation_task = asyncio.create_task(self._conversation_loop())
            
            # å‘é€åˆå§‹è§†è§‰ä¸Šä¸‹æ–‡
            await self._update_visual_context()
            
            # å‘é€æ¬¢è¿æ¶ˆæ¯
            await self._send_welcome_message()
            
            logger.info("ğŸ‰ Realtime conversation agent started successfully!")
            logger.info("ğŸ’¬ You can now speak naturally - the AI can see and hear you!")
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            await asyncio.gather(self.visual_update_task, self.conversation_task)
            
        except Exception as e:
            logger.error(f"Failed to start conversation agent: {e}")
            await self.stop_conversation()
            raise
    
    async def _send_welcome_message(self):
        """å‘é€æ¬¢è¿æ¶ˆæ¯"""
        try:
            welcome_text = (
                "Hello! I'm your realtime AI Construction copilot. I can see through your camera "
                "and hear through your microphone. Feel free to ask me anything on our journey together"
            )
            
            await self.ai_client.send_text_message(welcome_text)
            
        except Exception as e:
            logger.error(f"Failed to send welcome message: {e}")
    
    async def _visual_context_loop(self):
        """è§†è§‰ä¸Šä¸‹æ–‡æ›´æ–°å¾ªç¯"""
        while self.is_running:
            try:
                await self._update_visual_context()
                await asyncio.sleep(self.visual_context_interval)
            except Exception as e:
                logger.error(f"Error in visual context loop: {e}")
                await asyncio.sleep(1)
    
    async def _conversation_loop(self):
        """ä¸»å¯¹è¯å¾ªç¯"""
        while self.is_running:
            try:
                # è¿™é‡Œå¤„ç†æŒç»­çš„å¯¹è¯é€»è¾‘
                # ä¸»è¦å·¥ä½œç”±å›è°ƒå‡½æ•°å¤„ç†
                await asyncio.sleep(0.1)
            except Exception as e:
                logger.error(f"Error in conversation loop: {e}")
                await asyncio.sleep(1)
    
    async def _update_visual_context(self):
        """Update visual context from camera or screen"""
        try:
            current_frame = self.visual_input.get_current_frame()
            if current_frame is not None:
                # Convert to base64
                base64_image = self.visual_input.frame_to_base64(current_frame)
                self.current_visual_context = base64_image
                self.last_visual_update = time.time()
                
                # Send visual context to AI (silent update, no response required)
                await self._send_visual_context_update(base64_image)
                
                logger.debug(f"Visual context updated from {self.input_source}")
        except Exception as e:
            logger.error(f"Error updating visual context: {e}")
    
    async def _send_visual_context_update(self, base64_image: str):
        """å‘é€è§†è§‰ä¸Šä¸‹æ–‡æ›´æ–°ï¼ˆé™é»˜ï¼‰"""
        try:
            # å‘é€å›¾åƒä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä¸è¦æ±‚å›åº”
            context_message = {
                "type": "conversation.item.create",
                "previous_item_id": None,
                "item": {
                    "type": "message",
                    "role": "user",
                    "content": [
                        {
                            "type": "input_text", 
                            "text": "[Visual context update - no response needed]"
                        },
                        {
                            "type": "input_image", 
                            "image_url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    ]
                }
            }
            
            await self.ai_client.websocket.send(json.dumps(context_message))
            
        except Exception as e:
            logger.error(f"Error sending visual context: {e}")
    
    def _handle_speech_start(self):
        """å¤„ç†è¯­éŸ³å¼€å§‹"""
        logger.info("ğŸ¤ Speech detected - listening...")
        self.conversation_active = True
        
        # é‡ç½®éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = []
        self.current_audio_samples = 0
        self.is_processing_audio = False
        
        # å¦‚æœ AI æ­£åœ¨è¯´è¯ï¼Œåœæ­¢å®ƒ
        # TODO: å®ç°æ‰“æ–­æœºåˆ¶
    
    def _handle_speech_end(self):
        """å¤„ç†è¯­éŸ³ç»“æŸ"""
        logger.info("ğŸ¤ Speech ended - processing...")
        
        # å¤„ç†ç§¯ç´¯çš„éŸ³é¢‘æ•°æ®
        if self.audio_buffer and not self.is_processing_audio:
            self._process_accumulated_audio()
    
    def _handle_audio_data(self, audio_data: bytes):
        """å¤„ç†éŸ³é¢‘æ•°æ® - ç§¯ç´¯éŸ³é¢‘ç›´åˆ°æœ‰è¶³å¤Ÿçš„é•¿åº¦"""
        try:
            if self.conversation_active and not self.is_processing_audio:
                # ç§¯ç´¯éŸ³é¢‘æ•°æ®
                self.audio_buffer.append(audio_data)
                
                # ä¼°ç®—éŸ³é¢‘æ ·æœ¬æ•° (å‡è®¾16ä½å•å£°é“)
                samples_in_chunk = len(audio_data) // 2
                self.current_audio_samples += samples_in_chunk
                
                # å¦‚æœç§¯ç´¯äº†è¶³å¤Ÿçš„éŸ³é¢‘ï¼Œå¤„ç†å®ƒ
                if self.current_audio_samples >= self.min_audio_samples:
                    self._process_accumulated_audio()
                    
        except Exception as e:
            logger.error(f"Error handling audio data: {e}")
    
    def _process_accumulated_audio(self):
        """å¤„ç†ç§¯ç´¯çš„éŸ³é¢‘æ•°æ®"""
        try:
            if not self.audio_buffer or self.is_processing_audio:
                return
                
            # é˜²æ­¢é‡å¤å¤„ç†
            self.is_processing_audio = True
            
            # åˆå¹¶æ‰€æœ‰éŸ³é¢‘æ•°æ®
            combined_audio = b''.join(self.audio_buffer)
            
            logger.info(f"Processing {len(combined_audio)} bytes of audio ({self.current_audio_samples} samples)")
            
            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹å¼å‘é€åˆ° AI
            if self.main_loop and self.main_loop.is_running():
                future = asyncio.run_coroutine_threadsafe(
                    self._process_audio_input(combined_audio),
                    self.main_loop
                )
                logger.debug("Accumulated audio processing scheduled")
            else:
                logger.warning("Main event loop not available for audio processing")
                self.is_processing_audio = False
                
        except Exception as e:
            logger.error(f"Error processing accumulated audio: {e}")
            self.is_processing_audio = False
    
    async def _process_audio_input(self, audio_data: bytes):
        """å¤„ç†éŸ³é¢‘è¾“å…¥"""
        try:
            # å‘é€éŸ³é¢‘åˆ° AI
            await self.ai_client.send_audio_data(audio_data)
            
            # å¦‚æœæœ‰å½“å‰è§†è§‰ä¸Šä¸‹æ–‡ï¼Œä¸€èµ·å‘é€
            if self.current_visual_context:
                await self._send_current_visual_context_with_audio()
            
            # æäº¤éŸ³é¢‘å¹¶è¯·æ±‚å›åº”
            await self.ai_client.commit_audio_input()
            
            logger.info("âœ… Audio input processed and committed")
            
        except Exception as e:
            logger.error(f"Error processing audio input: {e}")
        finally:
            # é‡ç½®å¤„ç†çŠ¶æ€
            self.is_processing_audio = False
            self.audio_buffer = []
            self.current_audio_samples = 0
    
    async def _send_current_visual_context_with_audio(self):
        """éšè¯­éŸ³ä¸€èµ·å‘é€å½“å‰è§†è§‰ä¸Šä¸‹æ–‡"""
        try:
            if self.current_visual_context:
                context_message = {
                    "type": "conversation.item.create",
                    "previous_item_id": None,
                    "item": {
                        "type": "message",
                        "role": "user",
                        "content": [
                            {
                                "type": "input_text", 
                                "text": "Here's what I'm currently seeing:"
                            },
                            {
                                "type": "input_image", 
                                "image_url": f"data:image/jpeg;base64,{self.current_visual_context}"
                            }
                        ]
                    }
                }
                
                await self.ai_client.websocket.send(json.dumps(context_message))
                logger.debug("Visual context sent with audio")
                
        except Exception as e:
            logger.error(f"Error sending visual context with audio: {e}")
    
    def _handle_ai_text_response(self, text: str):
        """å¤„ç† AI æ–‡æœ¬å›åº”"""
        logger.info(f"ğŸ¤– AI (text): {text}")
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ–‡æœ¬æ˜¾ç¤ºé€»è¾‘
    
    def _handle_ai_audio_response(self, audio_data: bytes):
        """å¤„ç† AI éŸ³é¢‘å›åº”"""
        logger.info(f"ğŸ¤– AI (audio): {len(audio_data)} bytes")
        # æ’­æ”¾éŸ³é¢‘å›åº”
        self.audio_handler.handle_audio_response(audio_data)
    
    def _handle_ai_error(self, error_msg: str):
        """å¤„ç† AI é”™è¯¯"""
        logger.error(f"ğŸš¨ AI Error: {error_msg}")
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é”™è¯¯æ¢å¤é€»è¾‘
    
    async def stop_conversation(self):
        """åœæ­¢å¯¹è¯"""
        logger.info("Stopping realtime conversation agent...")
        
        self.is_running = False
        
        # å–æ¶ˆä»»åŠ¡
        if self.visual_update_task:
            self.visual_update_task.cancel()
        if self.conversation_task:
            self.conversation_task.cancel()
        
        # Stop components
        if self.visual_input:
            self.visual_input.stop_capture()
        if self.microphone:
            self.microphone.stop_capture()
        if self.audio_handler:
            self.audio_handler.stop()
        if self.ai_client:
            await self.ai_client.disconnect()
        
        logger.info("âœ… Realtime conversation agent stopped")
    
    async def ask_about_vision(self, question: str):
        """è¯¢é—®å…³äºå½“å‰è§†è§‰å†…å®¹çš„é—®é¢˜"""
        try:
            if not self.current_visual_context:
                await self._update_visual_context()
            
            if self.current_visual_context:
                await self.ai_client.send_image_for_analysis(
                    self.current_visual_context, 
                    question
                )
                logger.info(f"Asked about vision: {question}")
            else:
                logger.warning("No visual context available")
                
        except Exception as e:
            logger.error(f"Error asking about vision: {e}")


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        print("âŒ Please set OPENAI_API_KEY in your .env file")
        return
    
    agent = RealtimeConversationAgent(api_key)
    
    try:
        await agent.start_conversation()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Goodbye!")
    finally:
        await agent.stop_conversation()


if __name__ == "__main__":
    asyncio.run(main())